---
title: "Assignment 03 Regressions "
subtitle: "BQOM 2578 | Data Mining"
date: "10/5/2025"
date-format: "full"
author: "Theresa Wohlever"
editor: source
format:
  pdf:
    toc: true
    toc-depth: 2
    number-sections: false
    mainfont: "Georgia"
    sansfont: "Avenir"
    monofont: "Menlo"
    monofontoptions: "Scale=0.6"
    mathfont: "STIX Two Math"
    pdf-engine: xelatex
---

# Executive Summary

How well can we predict county Suicide Rates from the hospital information on a per county basis within Pennsylvania? Combine both county Suicide rate data with all PA hospital data to address this question. Dependent Variable is the County Suicide rate.
The Data preparation includes removing a large number of features expected to be unrelated to suicide rates, changing the representation of categorical variables to integers, and joining hospital data and county level suicide data.

There are Linear regression models to predict county suicide rates. These include single feature models as well as stepwaise models. There are also Logistic regression models.

Single Feature Linear Regression performed for every Independent Variable (Feature) selected as relevant for this analysis. Of those, the highest R^2 (best) single-predictor model uses the variable discharges1864 with R^2 = 0.06481609

Of the three Stepwise Linear Regression models attempted, all three performed better than the Single Feature Models. Of these three, the backwards stepwise model predicted County Suicide Rates best with an Adjusted R-squared value of 0.1634. In all linear regression cases, these R^2 values are quite low; even though the explanatory power for the backwards stepwise linear regression is the best of all linear regression models, it can only explain 16% of the variance in county suicide rates.


Logistic regression provides insight into the impact of included features. Selected features were those shown to demonstrate significant impact on the linear regression. Using these features in a Logistic Regression we can better percieve their impact on County Suicide Rates.

1. `children_hospital`: Very large effect (β = 2.849)
2. `psych_over17`: Small effect, but not significant (β = 0.184)
3. `discharges1864`: Very small but significant effect (β = 0.0001)
4. `psych_over17_beds_lic`: Negligible effect (β = 0.001)

These findings are discussed in detail in [Logistic Regression Model 2 (Multi-Variable) : Beta Coefficients Discussion](#logistic-model-discussion).


# Data Preparation

```{r}
#| label: LoadPackages
#| echo: false
#| message: false
#| results: 'hide'
#| warning: false


rm(list = ls())

# library(caTools)
# library(ROCR)
# library(caret)
library(tidyverse)
library("tidyr") #pivot_longer funciton
library(corrplot)
library(tidyverse)
# library("lm.beta")

working_directory <- "/Users/theresawohlever/git_repos/BQOM-2578_DataMining/BQOM-2578_DataMining_twohlever/assignments/03-Regressions"
setwd(working_directory)

CSV_base_filename <- "hospital extract 2023"
CSV_IN_FILE <- paste(working_directory, "/", "raw_data/", CSV_base_filename, ".csv", sep = "")
CSV_OUT_FILE <- paste(working_directory, "/", CSV_base_filename, "_processed.csv", sep = "")

CSV_S_base_filename <- "SuicideByCounty"
CSV_S_IN_FILE <- paste(working_directory, "/", "raw_data/", CSV_S_base_filename, ".csv", sep = "")
CSV_S_OUT_FILE <- paste(working_directory, "/", CSV_S_base_filename, "_processed.csv", sep = "")

```

## Importing Data, Cleaning, & Wrangling

```{r}
#| label: ImportData
#| echo: false
#| message: false
#| results: 'hide'
#| warning: false

# read.csv will read the csv into a dataframe, which we can manipulate in R.
raw_df = read.csv(CSV_IN_FILE, stringsAsFactors = TRUE)
head(raw_df)
summary(raw_df)


raw_s_df = read.csv(CSV_S_IN_FILE, stringsAsFactors = TRUE)
head(raw_s_df)
summary(raw_s_df)


```


```{r}
#| label: CreateTarget
#| echo: false
#| message: true
#| warning: false

selected_cols <- c("facility_id", 
               "facility_county",
               "type_of_organization",
               "children_hospital",
               "hospital_ltc",
               "on_site_ltc",
               "privateroomexist",
               "semiprivateroomexist",
               "discharges1864",
               # "dischargestotal",
               "alcohol_drug_detox",
               "alcoholdetox_patient_days",
               "alcoholdetox_beds_lic_vs_staf",
               "alcoholdetox_adm_vs_pat_days",
               "alcohol_drug_treat",
               "alcoholtreat_closing_date",
               "alcoholtreat_beds_lic",
               "alcoholtreat_patient_days",
               "alcoholtreat_beds_lic_vs_staf",
               "alcohol_drug_treat_adm_vs_pat_days",
               "comprehensive_rehab",
               "comprehensive_rehab_beds_lic",
               "Comprehensive_rehab_patient_days",
               "comprehensive_rehab_beds_lic_vs_staf",
               "psych_0to17",
               "psych_0to17_beds_lic",
               "psych_0to17_patient_days",
               "psych_0to17_beds_lic_vs_staf",
               "psych_over17",
               "psych_over17_beds_lic",
               "psych_over17_patient_days",
               "psych_over17_beds_lic_vs_staf",
               "detox",
               "clinpsyc",
               "clinic_psychiatric",
               "psychiatrists"# ,
               # "ft_staff"
               )
# Keep only columns related to model 
## Only select columns that actually exist in the raw data
selected_cols <- selected_cols[!selected_cols %in% setdiff(selected_cols, names(raw_df)) ]
df <- subset(raw_df, select = selected_cols )

#
## Combine with county level suicide data
#
df <- merge(df, raw_s_df, by.x = "facility_county", by.y = "CountyState")
df_merged <- df ## Save state


# Only Keep relevant columns for predicting county suicide rate 
## Only select columns that actually exist in the raw data
selected_cols <- selected_cols[!selected_cols %in% c("facility_county", "CountyState")]
selected_cols <- c(selected_cols,"Obs_Count")
selected_cols <- selected_cols[!selected_cols %in% setdiff(selected_cols, names(df)) ]
df <- subset(df, select = selected_cols )


####
#
# Clean Up Values
#
####
df <- df %>% rename(target = Obs_Count)

Cols2Numeric <- c(
  "target"
)
# Remove formatting from character strings like commas or currency symbols
df <- df %>%
  mutate(across(.cols = all_of(Cols2Numeric), 
    ~ as.numeric(gsub("[^0-9.]", "", as.character(.)))))
df <- df %>%
  mutate(across(.cols = all_of(Cols2Numeric),
    ~ as.numeric(.)
  ))


## Convert Columns with Yes/No values to 0/1 Values
YesNo2Numeric <- df_merged %>%
   select(where(~ any(str_detect(as.character(.), regex("yes", ignore_case = TRUE)), na.rm = TRUE))) %>%
   names()

df <- df %>%
  mutate(across(.cols = all_of(YesNo2Numeric),
    ~ as.numeric(factor(.,
      levels = c("No", "Yes"))) - 1
  ))

## Convert other columns with strings to numbers
Cols2Factors <- c(
  "type_of_organization",
  "clinic_psychiatric"
)
# Convert character strings to Factors to Ints
df$type_of_organization <- as.integer(factor(df_merged$type_of_organization))


# Unique number already the initial character for clinic_psychiatric col
df <- df %>%
  mutate(clinic_psychiatric = str_sub(clinic_psychiatric, 1, 1))
df$clinic_psychiatric <- as.integer(factor(df_merged$clinic_psychiatric))  

```

### Data Review

Numeric variables are continuous.

Integer variables are categorical.

```{r}
#| label: ReviewData
#| echo: true
#| message: true
#| warning: false
#| fig-width: 10
#| fig-height: 10

##
## Variable TYPES
##

# sapply(df, typeof)
sapply(df, class)




##
## Visualize target values 
##

# Histogram of target values
target_density <- density(df$target)

# Convert the density estimate to a function
dens_func <- approxfun(target_density$x, target_density$y)

# Use optimize() to find the minimum in a specified interval (choose based on your data)
result <- optimize(dens_func, interval = c(min(df$target), max(df$target)))
local_min_x <- result$minimum     # The x value where local minimum occurs
local_min_y <- result$objective   # The minimum density value

#  Create density plot with ggplot2 and add vertical line at minimum
df_density <- data.frame(x = df$target)
ggplot(df_density, aes(x = df$target)) +
  geom_density(fill = "lightblue", color = "blue", alpha = 0.5) +
  geom_vline(xintercept = local_min_x , color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = local_min_x, y = local_min_y + 0.002, 
    label = sprintf("Min: %.0f", local_min_x), color = "red", angle = 90, vjust = -1, size = 6) +
  labs(title = "Density plot of County Observed Suicide rates in PA", x = "County Suicide Rate per 100,000", y = "Density") +
  coord_cartesian(ylim = c(0, 0.01)) 


## Make TARGET binary for logistic regression
target_bin_cutoff <- local_min_x 

```

Set the cut-off value for 1 or 0 (binary) for Logistic regression is the local minimum of county Suicide Rates.


```{r}
#| label: VisualizeAllVariables
#| echo: true
#| message: true
#| warning: false
#| fig-width: 10
#| fig-height: 10

##
## Visualize ALL data across target values
## Normalized Scatter
##
minMax <- function(x) { (x - min(x)) / (max(x) - min(x))}
df_norm <- as.data.frame(lapply(df, minMax))
df_long <- pivot_longer(df_norm, cols = -target, names_to = "variable", values_to = "value")
ggplot(df_long, aes(x = target, y = value, color = variable)) +
  geom_point(size = 2) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 7, title = "Feature")) +
  labs(x = "", y = "", color = "Feature")

```

No clear clustering patterns across all variables vs. County Suicide Rates (Regression Target).

## Preliminary Analysis

Evaluate Correlation Matrix

```{r}
#| label: CorrelationMatrix
#| echo: true
#| message: true
#| warning: false
#| fig-width: 8
#| fig-height: 10

## Prep for correlation
df_cor <- df

cor_mat <- cor(df)
cor_threshold <- 0
cor_threshold_count <- 2

cols_above_threshold <- which( colSums(abs(cor_mat) > cor_threshold, na.rm = TRUE) >= cor_threshold_count)
df <- subset(df, select = colnames(cor_mat)[cols_above_threshold] )
cor_mat <- cor(df)

cat(paste(colnames(cor_mat)[cols_above_threshold], collapse = "\n"))

cor_mat_plot <- round(cor_mat, 2)
cor_mat_plot[is.na(cor_mat_plot)] <- 0 # Replace all NA values with zero
corrplot(cor_mat_plot, 
  method="square",
  type="upper",
  order="AOE", 
  tl.col="darkgrey",
  cl.align.text = "r",
  diag=FALSE, 
  number.cex=0.6)


```

# Regression

## Linear Regression by R Value

Bake off by R squared

```{r}
#| label: LinearRegressionSingePredictorBakeOff
#| warning: false
#| echo: true
#| message: true

response <- "target"
predictors <- setdiff(names(df), response)

best_r2 <- -Inf
best_model <- NULL
best_predictor <- NULL

for (predictor in predictors) {
  formula <- as.formula(paste(response, "~", predictor))
  model <- lm(formula, data = df)
  r2 <- summary(model)$r.squared

  if (r2 > best_r2) {
    best_r2 <- r2
    best_model <- model
    best_predictor <- predictor
  }
}

cat("Best single-predictor model uses:", best_predictor, "with R^2 =", best_r2, "\n")

# To view details of best_model:
summary(best_model)



```

## Stepwise Linear Regression

```{r}
#| label: StepwiseRegression
#| warning: false
#| echo: true
#| message: true

# For more info: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/step 

model <- lm(target ~ ., data = df)
summary(model)

# Perform stepwise regression
step_model_back <- step(model, direction = "backward",trace=0)
summary(step_model_back)

step_model_forward <- step(model, direction = "forward",trace=0)
summary(step_model_forward)


```



## Logistic Regression

```{r}
#| label: LogisticRegression
#| warning: false
#| echo: true
#| message: true

# Update target to be binomial
df$target <- ifelse(df$target < target_bin_cutoff, 0, 1)

# Logistic Regression with ONLY discharges1864
logreg_ch <- glm(target ~ discharges1864, data=df, family="binomial")
summary(logreg_ch)

# Logistic Regression with discharges1864 + 
#  children_hospital + psych_over17  + psych_over17_beds_lic
logreg_chdisch <- glm(target ~ discharges1864  +  children_hospital + psych_over17
             + psych_over17_beds_lic, data=df, family="binomial")
summary(logreg_chdisch)


logreg <- logreg_chdisch
coeftable <- data.frame(col1=coef(logreg),col2=exp(coef(logreg)))
colnames(coeftable)<-c('Coefficient (log-odds)','e^coefficient (odds)')
coeftable

#
# Confusion Matrix
#
df$PredLogOdds <- df$PredProbs <- predict(logreg, newdata=df)
df$PredProbs <- predict(logreg, newdata=df, type="response")  
# type="response" gives the probability
summary(df$PredProbs)



# transform prediction into either 1 (Elevated Suicide Rate) or 0 (Suicide Rate) using a cutoff point
for (cutoff in c(0.25, 0.15, 0.1, 0.05))
{
  df$PredHighSuicide <- ifelse(df$PredProbs >= cutoff,1,0)
  summary(df$PredHighSuicide)
  cat(paste("For a cutoff point of", 
    cutoff, "the proportion of Counties predicted to have high suicide rates is", 
    round(mean(df$PredHighSuicide),2)), "\n\n")
}

```




Of the two Logistic Regression models, **Model 2 (Multi-Variable)** demonstrates superior statistical and explanatory power.

### Logistic Regression Model Comparison

| Metric | Model 1 (Single Variable) | Model 2 (Multi-Variable) |
|--------|---------------------------|--------------------------|
| **Variables** | discharges1864 only | discharges1864 + children_hospital + psych_over17 + psych_over17_beds_lic |
| **AIC** | 225.17 | 216.84 |
| **Deviance Explained** | 3.77% | 10.00% |
| **Significant Predictors** | 1 out of 1 | 2 out of 4 |
| **Residual Deviance** | 221.17 | 206.84 |

### Model 2 (Multi-Variable) Demonstrates Superior Statistical Power

#### Lower AIC Score
Model 2 achieves an AIC of 216.84 compared to Model 1's 225.17, representing an **8.33-point improvement**. The AIC penalizes model complexity while rewarding goodness of fit, so this reduction indicates that the additional variables provide meaningful explanatory value that outweighs their complexity cost.

#### Greater Deviance Explained
Model 2 explains **10.00% of the deviance** compared to Model 1's 3.77%. This represents a **2.65-fold increase** in explanatory power, indicating that the multi-variable model captures significantly more variance in county suicide rates.

#### Stronger Statistical Significance
Model 2 includes two highly significant predictors (p < 0.001):
- **discharges1864**: z = 3.404, p = 0.000663
- **children_hospital**: z = 3.356, p = 0.000790

The children_hospital variable emerges as particularly powerful with an **odds ratio of 17.27**, meaning counties with children's hospitals have over 17 times higher odds of being classified as high suicide rate counties.


#### Model 2's Key Strengths

##### Clinical Relevance
The model incorporates healthcare infrastructure variables that have logical connections to suicide rates:
- Hospital discharge volume (discharges1864) reflects overall healthcare capacity
- Children's hospital presence may indicate regional healthcare centralization
- Psychiatric services availability directly relates to mental health resources


### Limitations and Considerations
#### Overall Predictive Capacity
Both models show *relatively low explanatory power* overall. Even the superior Model 2 only explains 10% of the deviance in suicide rates, indicating that:
- Hospital infrastructure variables alone are insufficient predictors
- Suicide rates are influenced by complex socioeconomic, demographic, and environmental factors not captured in this dataset
- Additional variables (unemployment rates, population density, mental health resources) might improve model performance

#### **Binary Classification Performance**
The confusion matrix analysis reveals classification challenges across different probability cutoffs:
- At 0.25 cutoff: 18% of counties predicted as high suicide rate
- At 0.15 cutoff: 83% of counties predicted as high suicide rate
- This suggests the model has limited practical utility for classification

### Logistic Regression Models Comparison: Conclusion

**Model 2 (Multi-Variable) demonstrates the strongest statistical and explanatory power** due to its:
- Significantly lower AIC score (8.33-point improvement)
- Higher deviance explained (10.00% vs 3.77%)
- Multiple significant predictors with meaningful effect sizes
- Better model fit while maintaining reasonable complexity

However, both models reveal the **fundamental challenge** of predicting county suicide rates from hospital infrastructure data alone. The low overall explanatory power suggests that effective suicide rate prediction requires incorporating broader socioeconomic, demographic, and mental health resource variables beyond hospital characteristics. The children_hospital variable emerges as the most powerful single predictor, with its massive odds ratio of 17.27 suggesting that healthcare centralization patterns may serve as important indicators of regional suicide risk patterns.





# Logistic Regression Model 2 (Multi-Variable) : Beta Coefficients Discussion {#logistic-model-discussion}

The beta coefficients (β) in logistic regression Model 2 represent the **change in log-odds of the outcome** (high suicide rate classification) for each one-unit increase in the predictor variable, holding all other variables constant.

## Statistical Significance Assessment
### Significant Predictors (p < 0.001)
#### children_hospital (Beta = 2.849)
- **Z-score**: 3.356, **p-value**: 0.000790
- **95% Confidence Interval**: [1.185, 4.513]
- **Interpretation**: This is the **strongest predictor** in the model with a very large effect size. Counties with children's hospitals have **17.27 times higher odds** of being classified as high suicide rate counties
- **Practical Significance**: The large beta coefficient suggests children's hospitals may serve as markers for healthcare centralization in counties with higher population density or urban characteristics

#### discharges1864 (Beta = 0.0001180)
- **Z-score**: 3.404, **p-value**: 0.000663
- **95% Confidence Interval**: [0.000050, 0.000186]
- **Interpretation**: Though the coefficient appears small, it's highly statistically significant. Each additional hospital discharge increases the odds by a factor of 1.0001
- **Practical Significance**: The cumulative effect becomes meaningful given the large range of discharge volumes across counties

### Non-Significant Predictors (p > 0.05)

####  psych_over17 (Beta = 0.1836)
- **Z-score**: 0.837, **p-value**: 0.403
- **95% Confidence Interval**: [-0.246, 0.614] (includes zero)
- **Interpretation**: Despite having a reasonable effect size (OR = 1.20), this variable lacks statistical significance, suggesting **no reliable relationship** with suicide rates

####  psych_over17_beds_lic (Beta = 0.001335)
- **Z-score**: 0.338, **p-value**: 0.735
- **95% Confidence Interval**: [-0.006, 0.009] (includes zero)
- **Interpretation**: This variable shows **no meaningful effect** (OR ≈ 1.00) and is clearly non-significant

## Key Insights

### Model Reliability
- **Two out of four predictors** are statistically significant, indicating the model has some predictive validity but also contains potentially redundant variables
- The **large standard errors** for non-significant variables (0.219 and 0.004 respectively) relative to their coefficients suggest these relationships are unstable

### Effect Size Hierarchy
1. `children_hospital`: Very large effect (β = 2.849)
2. `psych_over17`: Small effect, but not significant (β = 0.184)
3. `discharges1864`: Very small but significant effect (β = 0.0001)
4. `psych_over17_beds_lic`: Negligible effect (β = 0.001)

### Confidence Intervals
The **95% confidence intervals** reveal critical information about coefficient reliability:
- **Significant variables**: Confidence intervals exclude zero, confirming reliability
- **Non-significant variables**: Confidence intervals include zero, indicating the true effect could be null

## Practical Implications

### Model Parsimony Considerations
The presence of two non-significant variables suggests the model could be **simplified** by removing `psych_over17` and `psych_over17_beds_lic` without meaningful loss of predictive power. This would likely:
- Reduce model complexity
- Improve interpretability
- Potentially improve AIC score further

### Clinical and Policy Relevance
The **highly significant `children_hospital` coefficient** (β = 2.849) indicates that healthcare infrastructure patterns, particularly pediatric care centralization, may serve as important **proxy indicators** for broader county characteristics that influence suicide rates. This finding warrants further investigation into the underlying mechanisms connecting pediatric healthcare access and community mental health outcomes.

The significant but small **`discharges1864` coefficient** suggests that overall hospital utilization patterns provide incremental predictive value, potentially reflecting broader healthcare access or community health status differences across counties.


# References

Hospital Data: https://www.pa.gov/agencies/health/health-statistics/health-facilities/hospital-reports

Suicide by County Data: https://www.phaim.health.pa.gov/EDD/WebForms/DeathCntySt.aspx
