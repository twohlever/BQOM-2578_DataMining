---
title: "Week 6 - Classification Trees"
format: docx
editor: visual
---

## Loading packages

We need the same packages than before for decision trees:

-   [rpart](https://cran.r-project.org/web/packages/rpart/rpart.pdf)

-   [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf)

```{r}
# Load them (always)
library(rpart)
library(rpart.plot)
#And our usual package
library(tidyverse)
```

## Dataset

We are using **cps09mar** dataset, same as last week.

```{r}
#remember, save this file in the same folder you have cps09mar for this code to work properly.
getwd()
df = read.csv("cps09mar.csv", stringsAsFactors = TRUE)
df<-df%>%relocate(earnings)


#
#

summary(df)
```

Last week, we saw several variables influenced earnings; education, gender, hours worked all came as relevant.

Now, we are interested in learning what can predict that someone earns at least 6 figures; turning it into a classification problem.

```{r}
#Two ways of creating our new variable:  
df$sixfigures<-ifelse(df$earnings>=100000,1,0)
df %>% mutate(sixfigures = case_when(earnings>=100000 ~ 1, .default=0))

# See ?case_when to see how this can be a different approach than nested ifelse's 

#It is always a good practice to see the proportion of observations we have for each case
table(df$sixfigures)
prop.table(table(df$sixfigures))  # prop is "proportion"
```

Finally, let's split the dataset and check that both train and test have a similar proportion of at-least-six-figures earners:

```{r}
set.seed(1013, sample.kind = "Rejection")

#split the dataset leaving 80% of observations in the training dataset and 20% in the test dataset:
spl = sample(nrow(df),0.8*nrow(df))
head(spl)

# Now lets split our dataset into train and test:

train.df = df[spl,]
test.df = df[-spl,]

cat("\n For the train dataset: ")      #  \n is a "new line" printing control
prop.table(table(train.df$sixfigures))
cat("\n For the test dataset: ")
prop.table(table(test.df$sixfigures))

```

## First Classification Trees

```{r}
tree<-rpart(sixfigures ~ ., data=train.df, method="class",cp=0.05)
#simple graph:
#outcome is just the most common value (the mode)
prp(tree)
#nicer graph:
#outcome by default is the most common value, the proportion of successes (variable==1) and the % of observations  
rpart.plot(tree,digits=-2)
#We can use extra features as before:
?rpart.plot()
#outcome is the most common value, the proportion of successes (variable==1) and the % of observations  

# extra gives different values:
# as before, doing extra+100 keeps the proportion of observations
# "2" classification rate; number of right classifications/total observations.
# try rpart.plot with extra = 102


# "3" misclassification rate; number of wrong classifications/total observations.
# try rpart.plot with extra = 103

```

Of course, this first tree is not really useful for interpretation; we are predicting using the same variable, but it provides an easy to understand visualization.

Now let's get a more useful for prediction tree by removing earnings:

```{r}
tree<-rpart(sixfigures ~ .-earnings, data=train.df, method="class", cp=0.05)
rpart.plot(tree,digits=-2)

#retry with cp=0.0075 

```

Not the most exciting tree, although it tells us something about accuracy.

## Cross-Validation to get a good tree

What does cross-validation tell us?

```{r}
#pick a large enough minbucket you would trust the results and
#a cp value small enough we will get large trees for the cpplot

# how can we get some reference info to help us decide?

tree_cv = rpart(sixfigures ~ .-earnings,data=train.df, method="class",cp= 0.003 )
rpart.plot(tree_cv,digits=-2)

plotcp(tree_cv)
```

Let's see the larger trees. When do they stop making sense?

```{r}
tree2 = rpart(sixfigures ~ .-earnings,data=train.df, method="class",minbucket=30,cp=.003)
rpart.plot(tree2,digits=-2)


tree3 = rpart(sixfigures ~ .-earnings,data=train.df, method="class",minbucket=30,cp=.0015)
rpart.plot(tree3,digits=-2)


#what about an even larger cp?
```

Let's go back and fix a problem we have overlooked with as.factor(). To do that we will need to go back to the beginning, before the split, and make this key change to a number of variables that are not integer, but categorical.

```{r}
# put this code back at the top, before the split 
# to get these to be categorical factors
# then rerun the split and other req'd code blocks above
#
#df$marital <- as.factor(df$marital)
#df$region <- as.factor(df$region)
#df$race <- as.factor(df$race)
#

```

## Making predictions and comparing different trees

Before we do this, let's make the tree, tree2 and tree3 range in sizes from smaller to larger. Try cp=0.0032 for tree2 and cp=0.0015 for tree3 and run tree up at the top with the default (around 6 leaves).

```{r}
test.df$pred = predict(tree, newdata = test.df, type="class")
test.df$pred2 = predict(tree2, newdata = test.df, type="class")
test.df$pred3 = predict(tree3, newdata = test.df, type="class")
test.df<-test.df%>%relocate(earnings,pred,pred2,pred3)
head(test.df)
tail(test.df)

```

```{r}
library(caret)
cat("1st tree (using earnings):")
confusionMatrix(test.df$pred,as.factor(test.df$sixfigures), positive="1")
# Remember that this version of confusion matrix has "actual" in columns and
#    has the predicted in the rows.  Also we need to specify positive as 1
#    for the performance metrics (e.g., Sensitivity) to be what we expect

cat("2nd tree (using nine splits):")
confusionMatrix(test.df$pred2,as.factor(test.df$sixfigures), positive="1")

cat("3rd tree (using eleven splits):")
confusionMatrix(test.df$pred3,as.factor(test.df$sixfigures), positive="1")

acc_tree <- sum(test.df$pred == test.df$sixfigures) / nrow(test.df)
acc_tree2 <- sum(test.df$pred2 == test.df$sixfigures) / nrow(test.df)
acc_tree3 <- sum(test.df$pred3 == test.df$sixfigures) / nrow(test.df)
print(paste("Accuracy for tree:", round(acc_tree * 100, 2), "%"))
print(paste("Accuracy for tree2:", round(acc_tree2 * 100, 2), "%"))
print(paste("Accuracy for tree3:", round(acc_tree3 * 100, 2), "%"))
```

Seems like the third one is the best one.

## Comparing with logistic regressions

How would that compare with a logistic regression?

```{r}
reg1<-glm(sixfigures~education+hours+female+age,data=train.df,family='binomial')
summary(reg1)


reg2<-glm(sixfigures~education+hours+female+age+hours+marital,data=train.df,family='binomial')
summary(reg2)
```

Outcome is very different. We have to find a common metric. Common approaches are accuracy (in this case) or sensitivity or specificity, if they better fit your modeling needs.

```{r}
test.df$predregprobs1 = predict(reg1, newdata = test.df, type="response")
test.df$predregprobs2 = predict(reg2, newdata = test.df, type="response")

test.df$predreg1<-ifelse(test.df$predregprobs1>0.5,1,0)
test.df$predreg2<-ifelse(test.df$predregprobs2>0.5,1,0)

acc_reg <- sum(test.df$predreg1 == test.df$sixfigures) / nrow(test.df)
acc_reg2 <- sum(test.df$predreg2 == test.df$sixfigures) / nrow(test.df)
print(paste("Accuracy for regression 1:", round(acc_reg * 100, 2), "%"))
print(paste("Accuracy for regression 2:", round(acc_reg2 * 100, 2), "%"))


confusionMatrix(data=as.factor(test.df$predreg1),reference=as.factor(test.df$sixfigures), positive = "1")

confusionMatrix(data=as.factor(test.df$predreg2),reference=as.factor(test.df$sixfigures), positive = "1")



```

This time, classification trees won (!) Intuitions into why?

-   **Which model would you use for making predictions? How would you justify choosing it?**

-   **Suppose you want to convince others that pursuing higher education is important for economical success. Which model would you use?**

**Note: Classification Trees don't have ROC or AUC because they don't have threshold points.**

Instead, we can use a Loss Matrix.

## Loss Matrix

-   To give a greater penalty to false negatives (predicting as negative when it is positive) we increase the second value.
-   If you wanted to give a greater penalty to false positives you would have to increase the third value.

```{r}
lossmatrix = matrix(c(0,5,1,0), byrow=FALSE, nrow=2)
lossmatrix
```

Our old tree 2 model compared with a tree trained on the loss matrix:

```{r}
tree2new = rpart(sixfigures ~ .-earnings,data=train.df, method="class",minbucket=100,cp=0.0077)
rpart.plot(tree2new,digits=-2)


losstree2 = rpart(sixfigures ~ .-earnings,data=train.df, method="class",parms=list(loss=lossmatrix),minbucket=100,cp=0.0077)
rpart.plot(losstree2,digits=-2)
```

We see 1 more often because classifications as 1 happen more often.

Sure, accuracy will be lower but sensitivity will be much higher.

```{r}
cat("Previous one:")
confusionMatrix(test.df$pred2,as.factor(test.df$sixfigures), positive="1")

cat("New one:")
test.df$losspred2 = predict(losstree2, newdata = test.df, type="class")
confusionMatrix(test.df$losspred2,as.factor(test.df$sixfigures), positive="1")  

```
